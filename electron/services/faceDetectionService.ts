/**
 * PhotoMind - äººè„¸æ£€æµ‹æœåŠ¡
 *
 * åŠŸèƒ½ï¼š
 * 1. ç…§ç‰‡äººè„¸æ£€æµ‹ï¼ˆåŸºäº @vladmandic/face-apiï¼‰
 * 2. 128 ç»´äººè„¸ç‰¹å¾å‘é‡ç”Ÿæˆ
 * 3. æ‰¹é‡æ£€æµ‹ä¸è¿›åº¦è¿½è¸ª
 */
import { resolve } from 'path'
import { existsSync } from 'fs'
import { PhotoDatabase } from '../database/db.js'
import * as faceapi from '@vladmandic/face-api'
import * as tf from '@tensorflow/tfjs'
import '@tensorflow/tfjs-backend-cpu'
import sharp from 'sharp'
import { getEmbeddingService } from './hybridEmbeddingService.js'

export interface FaceDetectionResult {
  success: boolean
  detections: FaceInfo[]
  error?: string
  processingTimeMs: number
}

export interface FaceInfo {
  box: BoundingBox
  confidence: number
  landmarks?: FaceLandmarks
  descriptor?: number[]  // 128 ç»´äººè„¸ç‰¹å¾å‘é‡
}

export interface BoundingBox {
  x: number
  y: number
  width: number
  height: number
}

export interface FaceLandmarks {
  jawOutline: Point[]
  nose: Point[]
  mouth: Point[]
  leftEye: Point[]
  rightEye: Point[]
}

export interface Point {
  x: number
  y: number
}

export interface DetectionOptions {
  maxResults?: number
  minConfidence?: number
}

export interface BatchDetectionProgress {
  current: number
  total: number
  currentPhoto: string
  detectedFaces: number
  status: 'pending' | 'processing' | 'completed' | 'cancelled'
}

export interface FaceDetectionServiceConfig {
  modelsPath?: string
  minConfidence?: number
  maxFaces?: number
}

export class FaceDetectionService {
  private modelsPath: string
  private isLoaded = false
  private abortController: AbortController | null = null
  private minConfidence = 0.5
  private maxFaces = 10

  private tfBackendReady = false

  constructor(config?: FaceDetectionServiceConfig) {
    // ä½¿ç”¨ @vladmandic/face-api åŒ…å†…çš„æ¨¡å‹è·¯å¾„
    this.modelsPath = config?.modelsPath || resolve(
      process.cwd(),
      'node_modules/@vladmandic/face-api/model'
    )
    if (config?.minConfidence) this.minConfidence = config.minConfidence
    if (config?.maxFaces) this.maxFaces = config.maxFaces
  }

  private async ensureTfBackend(): Promise<void> {
    if (!this.tfBackendReady) {
      // å¼ºåˆ¶ CPU æ¨¡å¼ï¼Œé¿å… WebGL å…¼å®¹æ€§é—®é¢˜
      await tf.setBackend('cpu')
      await tf.ready()
      this.tfBackendReady = true
      console.log('[FaceDetection] TF.js åç«¯å·²è®¾ç½®ä¸º CPU æ¨¡å¼')

      // æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ³„éœ² tensor
      const numTensors = tf.memory().numTensors
      if (numTensors > 100) {
        console.warn(`[FaceDetection] æ£€æµ‹åˆ° ${numTensors} ä¸ªæœªé‡Šæ”¾çš„ tensorï¼Œæ‰§è¡Œæ¸…ç†`)
        tf.disposeVariables()
      }
    }
  }

  /**
   * åŠ è½½æ£€æµ‹æ¨¡å‹
   */
  async loadModels(): Promise<{ success: boolean; error?: string }> {
    if (this.isLoaded) {
      return { success: true }
    }

    try {
      // ç¡®ä¿ TensorFlow.js åç«¯å·²åˆå§‹åŒ–
      await this.ensureTfBackend()

      console.log('[FaceDetection] åŠ è½½ face-api.js æ¨¡å‹...')
      console.log(`[FaceDetection] ğŸ“ æ¨¡å‹è·¯å¾„: ${this.modelsPath}`)

      // æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
      const requiredModels = [
        'tiny_face_detector_model-weights_manifest.json',
        'face_landmark_68_model-weights_manifest.json',
        'face_recognition_model-weights_manifest.json'
      ]

      for (const model of requiredModels) {
        const modelPath = resolve(this.modelsPath, model)
        const exists = existsSync(modelPath)
        console.log(`[FaceDetection] ${exists ? 'âœ…' : 'âŒ'} ${model}`)
        if (!exists) {
          return { success: false, error: `æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: ${model}` }
        }
      }

      // å¹¶è¡ŒåŠ è½½æ‰€æœ‰æ¨¡å‹
      await Promise.all([
        faceapi.nets.tinyFaceDetector.loadFromDisk(this.modelsPath),
        faceapi.nets.faceLandmark68Net.loadFromDisk(this.modelsPath),
        faceapi.nets.faceRecognitionNet.loadFromDisk(this.modelsPath)
      ])

      this.isLoaded = true
      console.log('[FaceDetection] æ¨¡å‹åŠ è½½æˆåŠŸ')
      return { success: true }
    } catch (error) {
      console.error('[FaceDetection] æ¨¡å‹åŠ è½½å¤±è´¥:', error)
      return { success: false, error: String(error) }
    }
  }

  /**
   * æ£€æŸ¥æ¨¡å‹çŠ¶æ€
   */
  getModelStatus(): { loaded: boolean; modelsPath: string; configured: boolean } {
    return {
      loaded: this.isLoaded,
      modelsPath: this.modelsPath,
      configured: this.isLoaded
    }
  }

  /**
   * æ£€æµ‹ç…§ç‰‡ä¸­çš„äººè„¸
   */
  async detect(imagePath: string, options: DetectionOptions = {}): Promise<FaceDetectionResult> {
    const startTime = Date.now()
    const { minConfidence = this.minConfidence } = options

    console.log(`[FaceDetection] ğŸ¯ å¼€å§‹æ£€æµ‹: ${imagePath.split('/').pop()}`)

    // æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if (!existsSync(imagePath)) {
      console.error(`[FaceDetection] âŒ æ–‡ä»¶ä¸å­˜åœ¨: ${imagePath}`)
      return {
        success: false,
        detections: [],
        error: 'å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨',
        processingTimeMs: Date.now() - startTime
      }
    }

    try {
      // ç¡®ä¿æ¨¡å‹å·²åŠ è½½
      const loadResult = await this.loadModels()
      if (!loadResult.success) {
        console.error(`[FaceDetection] âŒ æ¨¡å‹åŠ è½½å¤±è´¥: ${loadResult.error}`)
        return {
          success: false,
          detections: [],
          error: `æ¨¡å‹åŠ è½½å¤±è´¥: ${loadResult.error}`,
          processingTimeMs: Date.now() - startTime
        }
      }

      console.log(`[FaceDetection] ğŸ¤– æ¨¡å‹å°±ç»ªï¼Œå¼€å§‹å¤„ç†å›¾åƒ...`)

      // åŠ è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸º tensor
      const { data, info } = await sharp(imagePath)
        .raw()
        .ensureAlpha()
        .resize(416, 416, { fit: 'inside' })
        .toBuffer({ resolveWithObject: true })

      const { width, height } = info

      // å»æ‰ alpha é€šé“ (RGBA -> RGB)
      const rgbData = new Uint8Array(width * height * 3)
      for (let i = 0; i < width * height; i++) {
        rgbData[i * 3] = data[i * 4]       // R
        rgbData[i * 3 + 1] = data[i * 4 + 1] // G
        rgbData[i * 3 + 2] = data[i * 4 + 2] // B
      }

      const imageTensor = tf.tensor3d(rgbData, [height, width, 3])

      let detections: faceapi.WithFaceDescriptor<faceapi.WithFaceLandmarks<{
        detection: faceapi.FaceDetection
      }, faceapi.FaceLandmarks68>>[] = []

      try {
        // æ·»åŠ è¶…æ—¶æ§åˆ¶ï¼ˆ45ç§’ï¼‰
        const detectionPromise = faceapi
          .detectAllFaces(imageTensor, new faceapi.TinyFaceDetectorOptions({ inputSize: 416, scoreThreshold: minConfidence }))
          .withFaceLandmarks()
          .withFaceDescriptors()

        const timeoutPromise = new Promise<never>((_, reject) =>
          setTimeout(() => reject(new Error('äººè„¸æ£€æµ‹è¶…æ—¶ (45s)')), 45000)
        )

        detections = await Promise.race([detectionPromise, timeoutPromise])
        console.log(`[FaceDetection] ğŸ“Š åŸå§‹æ£€æµ‹ç»“æœ: ${detections.length} å¼ äººè„¸`)
      } finally {
        // ç¡®ä¿ tensor è¢«é‡Šæ”¾
        imageTensor.dispose()
      }

      // è½¬æ¢ä¸ºå†…éƒ¨æ ¼å¼ - è¿‡æ»¤æ‰æ²¡æœ‰ descriptor çš„äººè„¸
      const faces: FaceInfo[] = detections
        .filter(d => d.descriptor && d.descriptor.length === 128)  // ç¡®ä¿æœ‰æœ‰æ•ˆçš„128ç»´ç‰¹å¾å‘é‡
        .map(d => ({
          box: {
            x: d.detection.box.x,
            y: d.detection.box.y,
            width: d.detection.box.width,
            height: d.detection.box.height
          },
          confidence: d.detection.score,
          landmarks: this.convertLandmarks(d.landmarks),
          descriptor: Array.from(d.descriptor)  // Float32Array -> number[] (128ç»´)
        }))

      console.log(`[FaceDetection] ğŸ“Š æœ‰æœ‰æ•ˆæè¿°ç¬¦çš„äººè„¸: ${faces.length}/${detections.length}`)

      // é™åˆ¶æœ€å¤§äººè„¸æ•°
      const limitedFaces = faces.slice(0, this.maxFaces)

      console.log(`[FaceDetection] âœ… æ£€æµ‹å®Œæˆ: ${limitedFaces.length} å¼ äººè„¸ (${Date.now() - startTime}ms)`)

      return {
        success: true,
        detections: limitedFaces,
        processingTimeMs: Date.now() - startTime
      }
    } catch (error) {
      console.error('[FaceDetection] æ£€æµ‹å¤±è´¥:', error)
      return {
        success: false,
        detections: [],
        error: error instanceof Error ? error.message : 'æ£€æµ‹å¤±è´¥',
        processingTimeMs: Date.now() - startTime
      }
    }
  }

  /**
   * è½¬æ¢ face-api.js çš„ landmarks åˆ°å†…éƒ¨æ ¼å¼
   */
  private convertLandmarks(landmarks: faceapi.FaceLandmarks68): FaceLandmarks {
    const positions = landmarks.positions

    return {
      jawOutline: positions.slice(0, 17).map(p => ({ x: p.x, y: p.y })),
      nose: positions.slice(27, 36).map(p => ({ x: p.x, y: p.y })),
      mouth: positions.slice(48, 68).map(p => ({ x: p.x, y: p.y })),
      leftEye: positions.slice(36, 42).map(p => ({ x: p.x, y: p.y })),
      rightEye: positions.slice(42, 48).map(p => ({ x: p.x, y: p.y }))
    }
  }

  /**
   * æ‰¹é‡æ£€æµ‹
   */
  async detectBatch(
    imagePaths: string[],
    options: DetectionOptions = {},
    onProgress?: (progress: BatchDetectionProgress) => void
  ): Promise<{
    results: Map<string, FaceDetectionResult>
    totalDetected: number
    processingTimeMs: number
  }> {
    const startTime = Date.now()
    const results = new Map<string, FaceDetectionResult>()
    let totalDetected = 0

    // è®¾ç½®å–æ¶ˆæ§åˆ¶å™¨
    this.abortController = new AbortController()

    for (let i = 0; i < imagePaths.length; i++) {
      // æ£€æŸ¥æ˜¯å¦å–æ¶ˆ
      if (this.abortController?.signal.aborted) {
        console.log('[FaceDetection] æ£€æµ‹ä»»åŠ¡å·²å–æ¶ˆ')
        break
      }

      const imagePath = imagePaths[i]
      const filename = imagePath.split('/').pop() || `photo_${i}`

      // æŠ¥å‘Šè¿›åº¦
      onProgress?.({
        current: i + 1,
        total: imagePaths.length,
        currentPhoto: filename,
        detectedFaces: totalDetected,
        status: 'processing'
      })

      // æ£€æµ‹
      const result = await this.detect(imagePath, options)
      results.set(imagePath, result)

      if (result.success) {
        totalDetected += result.detections.length
      }

      console.log(`[FaceDetection] ${i + 1}/${imagePaths.length}: ${filename} - ${result.detections.length} å¼ äººè„¸`)
    }

    // æŠ¥å‘Šå®Œæˆ
    onProgress?.({
      current: imagePaths.length,
      total: imagePaths.length,
      currentPhoto: '',
      detectedFaces: totalDetected,
      status: this.abortController?.signal.aborted ? 'cancelled' : 'completed'
    })

    return {
      results,
      totalDetected,
      processingTimeMs: Date.now() - startTime
    }
  }

  /**
   * å–æ¶ˆæ£€æµ‹ä»»åŠ¡
   */
  cancel(): void {
    this.abortController?.abort()
    console.log('[FaceDetection] æ£€æµ‹ä»»åŠ¡å·²å–æ¶ˆ')
  }

  /**
   * æ£€æµ‹å¹¶ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆåŒå‘é‡ç‰ˆæœ¬ï¼‰
   * åŒæ—¶ç”Ÿæˆ 128ç»´ face_embedding å’Œ 512ç»´ semantic_embedding
   */
  async detectAndSave(imagePath: string, database: PhotoDatabase): Promise<FaceDetectionResult> {
    const startTime = Date.now()

    // æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if (!existsSync(imagePath)) {
      console.warn(`[FaceDetection] æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: ${imagePath}`)
      return {
        success: false,
        detections: [],
        error: 'æ–‡ä»¶ä¸å­˜åœ¨',
        processingTimeMs: Date.now() - startTime
      }
    }

    const result = await this.detect(imagePath, {})

    if (result.success && result.detections.length > 0) {
      // è·å–ç…§ç‰‡
      const photo = database.getPhotoByFilePath(imagePath)

      if (!photo) {
        console.warn(`[FaceDetection] æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ç…§ç‰‡: ${imagePath}`)
        return result
      }

      // å®‰å…¨è·å– photoId
      const photoId = (photo as any).id ?? null

      if (!photoId) {
        console.warn(`[FaceDetection] ç…§ç‰‡ç¼ºå°‘ idï¼Œè·³è¿‡: ${imagePath}`)
        return result
      }

      // è·å–å›¾ç‰‡å°ºå¯¸ç”¨äºåæ ‡è½¬æ¢
      const imageInfo = await sharp(imagePath).metadata()
      const imgWidth = imageInfo.width || 1
      const imgHeight = imageInfo.height || 1

      // åŠ è½½ sharp å®ä¾‹ç”¨äºè£å‰ª
      const sharpInstance = sharp(imagePath)

      // ä¸ºæ¯ä¸ªäººè„¸ç”ŸæˆåŒå‘é‡
      const facesToSave: Array<{
        id: string
        bbox_x: number
        bbox_y: number
        bbox_width: number
        bbox_height: number
        confidence: number
        embedding?: number[]
        face_embedding?: number[]
        semantic_embedding?: number[]
        vector_version?: number
      }> = []
      for (let index = 0; index < result.detections.length; index++) {
        const face = result.detections[index]
        const faceId = `${photoId}_${index}_${Date.now()}`

        // è½¬æ¢ç›¸å¯¹åæ ‡ä¸ºç»å¯¹åƒç´ åæ ‡
        const absX = Math.round((face.box.x / 416) * imgWidth)
        const absY = Math.round((face.box.y / 416) * imgHeight)
        const absWidth = Math.round((face.box.width / 416) * imgWidth)
        const absHeight = Math.round((face.box.height / 416) * imgHeight)

        // ç¡®ä¿åæ ‡åœ¨å›¾ç‰‡èŒƒå›´å†…
        const cropX = Math.max(0, absX)
        const cropY = Math.max(0, absY)
        const cropWidth = Math.min(absWidth, imgWidth - cropX)
        const cropHeight = Math.min(absHeight, imgHeight - cropY)

        let semanticEmbedding: number[] | undefined = undefined

        try {
          // è£å‰ªäººè„¸åŒºåŸŸ
          const faceBuffer = await sharpInstance
            .clone()
            .extract({ left: cropX, top: cropY, width: cropWidth, height: cropHeight })
            .resize(224, 224, { fit: 'cover' })
            .jpeg({ quality: 90 })
            .toBuffer()

          // è½¬æ¢ä¸º base64 ä¼ é€’ç»™ CLIP
          const faceBase64 = `data:image/jpeg;base64,${faceBuffer.toString('base64')}`

          // ç”Ÿæˆ 512ç»´è¯­ä¹‰å‘é‡
          const embeddingService = getEmbeddingService()
          const clipResult = await embeddingService.imageToEmbedding(faceBase64)

          if (clipResult.success && clipResult.vector) {
            semanticEmbedding = clipResult.vector.values || (clipResult.vector as any)
            console.log(`[FaceDetection] ç”Ÿæˆ CLIP å‘é‡æˆåŠŸ: ${faceId}, ç»´åº¦: ${semanticEmbedding?.length}`)
          } else {
            console.warn(`[FaceDetection] CLIP å‘é‡ç”Ÿæˆå¤±è´¥: ${faceId}, é”™è¯¯: ${clipResult.error}`)
          }
        } catch (clipError) {
          console.warn(`[FaceDetection] ç”Ÿæˆè¯­ä¹‰å‘é‡å¤±è´¥: ${faceId}`, clipError)
          // ç»§ç»­ä¿å­˜ï¼Œsemantic_embedding å¯ä»¥ä¸º null
        }

        facesToSave.push({
          id: faceId,
          bbox_x: face.box.x,
          bbox_y: face.box.y,
          bbox_width: face.box.width,
          bbox_height: face.box.height,
          confidence: face.confidence,
          embedding: face.descriptor,  // 128 ç»´ face-api å‘é‡ (å…¼å®¹æ—§å­—æ®µ)
          face_embedding: face.descriptor,  // 128 ç»´ face-api å‘é‡
          semantic_embedding: semanticEmbedding,  // 512 ç»´ CLIP å‘é‡
          vector_version: semanticEmbedding ? 2 : 1  // 2=åŒå‘é‡, 1=åªæœ‰faceå‘é‡
        })
      }

      try {
        database.saveDetectedFaces(photoId, facesToSave)
        const withSemantic = facesToSave.filter(f => f.semantic_embedding).length
        console.log(`[FaceDetection] ä¿å­˜ ${facesToSave.length} å¼ äººè„¸: ${withSemantic} å¼ æœ‰è¯­ä¹‰å‘é‡, ${facesToSave.length - withSemantic} å¼ åªæœ‰äººè„¸å‘é‡`)
      } catch (e) {
        console.error(`[FaceDetection] ä¿å­˜äººè„¸å¤±è´¥: ${imagePath}`, e)
      }
    }

    return result
  }

  /**
   * æ£€æµ‹ç…§ç‰‡ä¸­çš„äººè„¸å¹¶è¿”å›è¾¹ç•Œæ¡†ï¼ˆç®€åŒ–ç‰ˆï¼‰
   */
  async detectFaces(imagePath: string): Promise<{
    faces: BoundingBox[]
    processingTimeMs: number
  }> {
    const result = await this.detect(imagePath)
    return {
      faces: result.detections.map(d => d.box),
      processingTimeMs: result.processingTimeMs
    }
  }

  /**
   * è·å–æ£€æµ‹ç»Ÿè®¡
   */
  getStats(): {
    modelLoaded: boolean
    configured: boolean
  } {
    return {
      modelLoaded: this.isLoaded,
      configured: this.isLoaded
    }
  }
}

export const faceDetectionService = new FaceDetectionService()
